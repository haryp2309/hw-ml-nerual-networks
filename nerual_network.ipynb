{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerual Network Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from helpers.hw2 import get_mnist_threes_nines, display_image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_exec(func):\n",
    "    '''Runs a function and removes the funciton scope'''\n",
    "    display(func())\n",
    "    del func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assertion Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSERT_OFF = False # Kill switch when we don't want assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def isarray(a, dim_len):\n",
    "    if ASSERT_OFF: return\n",
    "    if isinstance(dim_len, int):\n",
    "        dim_len = [dim_len]\n",
    "    assert isinstance(a, np.ndarray), f\"Expected np.array but was {type(a)}: {a}\"\n",
    "    assert len(a.shape) in dim_len, f\"Expected dimention {dim_len} but was {len(a.shape)}: {a}\"\n",
    "\n",
    "def test():\n",
    "    a = np.array([[1,2,3]])\n",
    "    #a = [1,2,3,4]\n",
    "    isarray(a, 2)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def iscallable(c, input_len, x=10):\n",
    "    if ASSERT_OFF: return\n",
    "    from inspect import signature\n",
    "    assert callable(c)\n",
    "    \n",
    "    actual_input_len = len([noe for noe in signature(c).parameters.values() if \"=\" not in str(noe)])\n",
    "    assert actual_input_len == input_len, f\"Expected {c.__name__} to take {input_len} arguments, but was {actual_input_len} instead.\"\n",
    "def test():\n",
    "    iscallable(iscallable, 2)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def isnum(i):\n",
    "    if ASSERT_OFF: return\n",
    "    assert isinstance(i, int) or isinstance(i, np.int64) or isinstance(i, float), f\"Expected to be int, but was {type(i)}: {i}\"\n",
    "\n",
    "def test():\n",
    "    #is_int(\"2\")\n",
    "    isnum(2)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def islist(l, content_type):\n",
    "    if ASSERT_OFF: return\n",
    "    \n",
    "    assert isinstance(l, list), f\"Expected to be list, but was {type(l)}: {l}\"\n",
    "    def check(content):\n",
    "        if content_type == \"callable\":\n",
    "            return callable(content)\n",
    "        if isinstance(content_type, tuple) and isinstance(content_type[0], type):\n",
    "            if content_type[0] == np.ndarray:\n",
    "                isarray(content, content_type[1])\n",
    "        return isinstance(content, content_type)\n",
    "    not_matching = [str(content) for content in l if not check(content)]\n",
    "    assert len(not_matching) == 0, f\"These elements are not of type {content_type}: {', '.join(not_matching)}\"\n",
    "def test():\n",
    "    #is_int(\"2\")\n",
    "    #islist([\"hei\", 2, \"bo\", 3], str)\n",
    "    islist([\"hei\", \"bo\"], str)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.2a (finite differences checker, used to help implement `my_nn_finite_difference_checker` in 1.3a. Feel free to modify the function signature, or to skip this part and implement `my_nn_finite_difference_checker` without this helper function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`finite_difference_checker`**\n",
    "\n",
    "Parameter 1 `f`: function\n",
    "\n",
    "Parameter 2 `x`: np.array\n",
    "\n",
    "Parameter 3 `k`: which $x_i$ to take partial derivative of\n",
    "\n",
    "Parameter `epsilon`: infinitively small value (approx)\n",
    "\n",
    "Returns: $\\frac{\\partial f}{\\partial x_{k+1}}(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def finite_difference_checker(f, x, k, epsilon = 10**(-5)):\n",
    "    \"\"\"Returns $\\frac{\\partial f}{\\partial x_k}(x)$\"\"\"\n",
    "    iscallable(f, 1)\n",
    "    isarray(x, 1)\n",
    "    isnum(k)\n",
    "    \n",
    "    minuend_x = x+np.identity(x.shape[0])[k]*epsilon\n",
    "    subtrahend_x = x-np.identity(x.shape[0])[k]*epsilon\n",
    "    return (f(minuend_x)- f(subtrahend_x))/(2*epsilon)\n",
    "\n",
    "def test():\n",
    "    f = lambda x: x**2\n",
    "    x = np.array([1,2,3,4])\n",
    "    k = 0\n",
    "    return finite_difference_checker(f, x, k)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`gradient_approx`**\n",
    "\n",
    "Parameter 1 f: function\n",
    "\n",
    "Parameter 2 x: np.array\n",
    "\n",
    "Parameter `epsilon`: infinitively small value (approx)\n",
    "\n",
    "Returns: $\\nabla_x f(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71828183,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  7.3890561 ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , 20.08553692,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , 54.59815003,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  2.71828183]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' def gradient_approx(f, x, epsilon = 10**(-5)):\n",
    "    x_matrix = np.full((x.size, x.size), x)\n",
    "    epsilon_matrix = np.identity(x.size) * epsilon\n",
    "    minuend_x = x_matrix+epsilon_matrix\n",
    "    subtrahend_x = x_matrix-epsilon_matrix\n",
    "    \n",
    "    return (f(minuend_x)- f(subtrahend_x))*(1/(2*epsilon)) '''\n",
    "\n",
    "def gradient_approx(f, x, epsilon = 10**(-5)):\n",
    "    iscallable(f, 1)\n",
    "    isarray(x, 1)\n",
    "    isnum(epsilon)\n",
    "    \n",
    "    fdc = lambda i: finite_difference_checker(f, x, i, epsilon=epsilon, )\n",
    "    vfdc = np.vectorize(fdc, signature=\"()->(n)\")\n",
    "    return vfdc(np.arange(x.shape[0]))\n",
    "    \n",
    "def test():\n",
    "    f = lambda x: np.exp(x)\n",
    "    x = np.array([1,2,3,4,1])\n",
    "    return gradient_approx(f, x)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.2b (functions that implement neural network layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sigmoid_activation`**\n",
    "\n",
    "Parameter 1 `x`: np.array\n",
    "\n",
    "Returns: (out: sigmoids output, grad: gradient of the sigmoid in respect of $x_i$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.31058579e-01, 8.80797078e-01, 9.52574127e-01, 1.00000000e+00,\n",
       "        1.00000000e-15]),\n",
       " array([[0.19661193, 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.10499359, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.04517666, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid_activation(x, epsilon = 10**(-15)):\n",
    "    # YOUR CODE HERE\n",
    "    isarray(x, 1)\n",
    "    isnum(epsilon)\n",
    "\n",
    "    positive_sigmoid = lambda x: 1 / (1 + np.exp(-x, where = x>=0))\n",
    "    sigmoid = lambda x: np.where(x >= 0, positive_sigmoid(x), 1-positive_sigmoid(-x))\n",
    "    #sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "    ''' inv_sigmoid = lambda y: -np.log((1/y ) - 1)\n",
    "    clipped_x = np.clip(sigmoid(x), inv_sigmoid(clip_start), inv_sigmoid(clip_end))  '''\n",
    "    clip_start = epsilon\n",
    "    clip_end = 1-epsilon\n",
    "    out = np.clip(sigmoid(x), clip_start, clip_end)\n",
    "\n",
    "    #grad = gradient_approx(sigmoid, x) # Disabled do to easier to calculate\n",
    "    grad = (sigmoid(x)*(1-sigmoid(x)))*np.identity(x.shape[0])\n",
    "    return out, grad\n",
    "\n",
    "def test():\n",
    "    x = np.array([1,2,3,np.int64(40**9), -1000])\n",
    "    return sigmoid_activation(x)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`logistic_loss`**\n",
    "\n",
    "Parameter 1 `g`: np.array of values\n",
    "\n",
    "Parameter 2 `y`: np.array of expected values\n",
    "\n",
    "Returns: (loss: array of losses, dL_dg: gradient of the loss with respect to `g`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.32508297, 0.50040242, 0.6108643 , 0.59191865]),\n",
       " array([[-3.29736238e-09,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 0.00000000e+00, -7.88258347e-10,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00, -3.05311332e-10,\n",
       "          0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          8.33333333e-01]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def logistic_loss(g, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for binary classification with logistic\n",
    "    loss\n",
    "\n",
    "    Inputs:\n",
    "    - g: Output of final layer with sigmoid activation,\n",
    "         of shape (n, 1) \n",
    "\n",
    "    - y: Vector of labels, of shape (n,) where y[i] is the label for x[i] \n",
    "         and y[i] in {0, 1}\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: array of losses\n",
    "    - dL_dg: Gradient of the loss with respect to g\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    isarray(g, 2)\n",
    "    isarray(y, 1)\n",
    "\n",
    "    g = g.T[0]\n",
    "    isarray(g, 1)\n",
    "\n",
    "    def loss_function (g): \n",
    "        g = np.clip(g, 1e-15, 1-1e-15) # To avoid 1+1e-15 and -1e-15\n",
    "        return -np.log((g**y) * ((1 - g)**(1-y)))\n",
    "\n",
    "    loss = loss_function(g)\n",
    "    dL_dg = gradient_approx(loss_function, g)\n",
    "    return loss, dL_dg\n",
    "\n",
    "def test():\n",
    "    g = np.array([[.1,.2,.3,.4]]).T\n",
    "    y = np.array([.1,.2,.3,.2])\n",
    "    return logistic_loss(g, y)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`relu_activation`**\n",
    "\n",
    "Parameter 1 `s`: np.array\n",
    "\n",
    "Returns: (out: relus output, ds: gradient of the relu in respect of $s_i$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 0, 4, 0]),\n",
       " array([[0. , 0. , 0. , 0. , 0. ],\n",
       "        [0. , 1. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 1. , 0. ],\n",
       "        [0. , 0. , 0. , 0. , 0.5]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu_activation(s):\n",
    "    # YOUR CODE HERE\n",
    "    isarray(s, 1)\n",
    "    \n",
    "    relu_function = lambda s: np.max([np.full(s.shape[0], 0), s], axis=0)\n",
    "    out = relu_function(s)\n",
    "    ds = gradient_approx(relu_function, s) # Disabled due to easyer to calculate\n",
    "    return out, ds\n",
    "\n",
    "def test():\n",
    "    s = np.array([-1,2,-3,4, 0])\n",
    "    return relu_activation(s)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To find $\\frac{\\partial l (W, b)}{\\partial W_{i,j}}$: \n",
    "> $$\\frac{\\partial l (W, b)}{\\partial w_{i,j}} = \\delta_j^{(l)}x_i^{(l-1)}= \\frac {\\partial e(w_{i,j})}{\\partial s_j^{(l)}} x_i^{(l-1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`layer_forward`**\n",
    "\n",
    "Parameter 1 `x`: np.array, $n \\times d^{(l-1)}$\n",
    "\n",
    "Parameter 2 `W`: np.array, $d^{(l-1)} \\times d^{(l)}$, contains all $w_{i,j}^{(l)}$ for this layer\n",
    "\n",
    "Parameter 3 `b`: np.array, $1 \\times d^{(l)}$ (matrix, not vector)\n",
    "\n",
    "Parameter 3 `activation_fn`: (in: np.array) -> (out: np.array, grad: np.array)\n",
    "\n",
    "Returns: (out: np.array ($n \\times d^{(l)}$) with next neurons, cache: (x: same as input,  da_dz: $\\frac{\\partial a}{\\partial z}$, W)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8, 19],\n",
       "        [ 5, 10],\n",
       "        [11, 19]]),\n",
       " (array([[1, 2, 3, 4],\n",
       "         [4, 2, 3, 4],\n",
       "         [4, 2, 6, 4]]),\n",
       "  array([[[1., 0.],\n",
       "          [0., 1.]],\n",
       "  \n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "  \n",
       "         [[1., 0.],\n",
       "          [0., 1.]]]),\n",
       "  array([[-1, -3],\n",
       "         [-4,  2],\n",
       "         [ 2,  3],\n",
       "         [ 2,  2]])))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def layer_forward(x, W, b, activation_fn):\n",
    "    # YOUR CODE HERE\n",
    "    isarray(x, 2)\n",
    "    isarray(W, 2)\n",
    "    isarray(b, 2)\n",
    "    iscallable(activation_fn, 1)\n",
    "    \n",
    "    n, _ = x.shape\n",
    "    b = b[0]\n",
    "    repeated_b = np.full((n, b.shape[0]), b)\n",
    "    act_fn_in = (x @ W) + repeated_b\n",
    "    v_activation_fn = np.vectorize(activation_fn, signature=\"(n)->(n), (n, n)\")\n",
    "    out, da_dz = v_activation_fn(act_fn_in)\n",
    "    cache = (x, da_dz, W)\n",
    "    return out, cache\n",
    "\n",
    "def test():\n",
    "    # n = 3\n",
    "    # d^(l-1) = 4\n",
    "    # d^(l) = 2\n",
    "    x = np.array([\n",
    "        [1, 2, 3, 4], \n",
    "        [4, 2, 3, 4],\n",
    "        [4, 2, 6, 4],\n",
    "    ])\n",
    "    W = np.array([\n",
    "        [-1, -3], \n",
    "        [-4, 2],\n",
    "        [2, 3],\n",
    "        [2, 2],\n",
    "    ])\n",
    "    b = np.array([[3, 1]])\n",
    "    activation_fn = relu_activation\n",
    "    return layer_forward(x, W, b, activation_fn)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.3b i, ii (deliverables for the sigmoid activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73 0.5  0.27]\n",
      "[[0.2  0.   0.  ]\n",
      " [0.   0.25 0.  ]\n",
      " [0.   0.   0.2 ]]\n",
      "================================================================================\n",
      "[1.e-15 1.e+00]\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 1.3b i\n",
    "s = np.asarray([1., 0., -1])\n",
    "out, grad = sigmoid_activation(s)\n",
    "with np.printoptions(precision=2):\n",
    "    print(out)\n",
    "    print(grad)\n",
    "    \n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1.3b ii\n",
    "s = np.asarray([-1000., 1000.])\n",
    "out, grad = sigmoid_activation(s)\n",
    "print(out)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.3b iii: What is the derivative of the negative log-likelihood loss with respect to $g$?\n",
    "> \n",
    "> **your answer here**\n",
    "\n",
    ">1.3b iv: Explain what is returned in `cache` in your `layer_forward` implementation. (Trying to answer this question before completing your implementation might help think about should go in `cache`, which should be stuff computed during the forward pass that is needed for backpropagation in the backward pass. Just make sure your final answer pertains to what you ultimately return in `cache`.)\n",
    ">\n",
    ">**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.2c (in this part you will code functions that initialize the neural network's weights. You will also code the forward pass which ties everything together, computing the output of a neural network with weights given by `weight_matrices` + `biases`, activation functions given by `activations`, on the input `X_batch`, a 2d input where each row is an individual input vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`create_weight_matrices`**\n",
    "\n",
    "Parameter 1 `layer_dims`: np.array, contains dimention of each layer in the neural network. `layer_dims[i]` defines\n",
    "      the number of neurons in the i+1 layer\n",
    "\n",
    "Returns: np.array, list of weight matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 9.62418473e-01, -2.35200248e+00,  1.00437977e+00,\n",
       "          1.03908306e+00],\n",
       "        [ 5.54588567e-01, -1.36129186e+00,  8.34660576e-01,\n",
       "          5.07040471e-04],\n",
       "        [ 5.36768149e-01,  3.40903668e+00,  4.54052129e-01,\n",
       "          1.26610067e+00],\n",
       "        [ 6.73983012e-01, -8.99932569e-01,  1.52008891e-01,\n",
       "          4.75192965e-01],\n",
       "        [-3.13269719e-01, -6.96115225e-01,  8.49764422e-01,\n",
       "          8.88587366e-01],\n",
       "        [-1.45182439e+00, -1.36431726e-01,  8.22917619e-01,\n",
       "          6.87855429e-01],\n",
       "        [ 5.58985498e-01, -3.06271987e-01,  1.26219326e-01,\n",
       "          5.77255025e-01]]),\n",
       " array([[-0.02013194],\n",
       "        [ 0.43121298],\n",
       "        [ 0.0156675 ],\n",
       "        [-0.83883607]])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_weight_matrices(layer_dims):\n",
    "    \"\"\"\n",
    "    Creates a list of weight matrices defining the weights of NN\n",
    "    \n",
    "    Inputs:\n",
    "    - layer_dims: A list whose size is the number of layers. layer_dims[i] defines\n",
    "      the number of neurons in the i+1 layer.\n",
    "\n",
    "    Returns a list of weight matrices\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    isarray(layer_dims, 1)\n",
    "    sizes=np.array([layer_dims[:-1], layer_dims[1:]]).T\n",
    "    weights = [np.random.normal(0,1,size) for size in sizes]\n",
    "    return weights\n",
    "\n",
    "def test():\n",
    "    #layer_dims = np.array([784, 200, 1])\n",
    "    layer_dims = np.array([7, 4, 1])\n",
    "    return create_weight_matrices(layer_dims)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`create_bias_vectors`**\n",
    "\n",
    "Parameter 1 `layer_dims`: np.array, contains dimention of each layer in the neural network. `layer_dims[i]` defines\n",
    "      the number of neurons in the i+1 layer\n",
    "\n",
    "Returns: np.array, list of weight biases for n-1 layers (first layer doesn't need bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01836862, -0.08424494,  1.5009134 , -0.75526176]]),\n",
       " array([[1.99907498]])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_bias_vectors(layer_dims):\n",
    "    # YOUR CODE HERE\n",
    "    isarray(layer_dims, 1)\n",
    "    biases = [np.random.normal(0,1,(1, size)) for size in layer_dims[1:]]\n",
    "    return biases\n",
    "\n",
    "def test():\n",
    "    #layer_dims = np.array([784, 200, 1])\n",
    "    layer_dims = np.array([7, 4, 1])\n",
    "    return create_bias_vectors(layer_dims)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`forward_pass`**\n",
    "\n",
    "Parameter 1: `X_batch`: np.array, matrix og shape ($n$, $d^{(1)}$)\n",
    "\n",
    "Parameter 2: `weight_matrices`: np.array, list of weight matrices, [$W^{(1)}, ..., W^{(\\max(l)-1)}$]\n",
    "\n",
    "Parameter 3: `biases`: np.array, list of bias vectors for each layer (without first layer), [$b^{(1)}$, ..., $b^{(\\max(l)-1)}$]\n",
    "\n",
    "Parameter 4: `activations`: np.array, list of activation funciton of each layer, size will be $\\max(l)-1$\n",
    "\n",
    "Returns: np.array, (vector of output, list of layer caches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.01000946],\n",
       "        [0.05939331],\n",
       "        [0.2461504 ]]),\n",
       " [(array([[1, 2, 3, 4, 8, 4, 1],\n",
       "          [4, 2, 3, 4, 8, 2, 9],\n",
       "          [4, 2, 6, 4, 2, 9, 1]]),\n",
       "   array([[[1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.],\n",
       "           [0., 0., 0., 1.]],\n",
       "   \n",
       "          [[1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.]],\n",
       "   \n",
       "          [[1., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0.]]]),\n",
       "   array([[ 0.4795968 , -0.32412168, -0.08872   , -0.88953954],\n",
       "          [ 0.43331034, -0.62320052, -0.58478363,  0.55919913],\n",
       "          [-1.01631523, -0.98634105, -0.448563  ,  0.70363832],\n",
       "          [-0.43549427,  1.08966881,  1.21891652,  0.07512697],\n",
       "          [ 0.46727646, -0.90424034, -1.23869719,  0.31595761],\n",
       "          [ 0.41406365, -1.9264484 , -1.5812752 , -0.7073796 ],\n",
       "          [ 1.37214123,  0.58686031, -1.03650839, -0.56970373]])),\n",
       "  (array([[ 4.62740518,  0.        ,  0.        ,  2.16473494],\n",
       "          [16.21519811,  0.        ,  0.        ,  0.        ],\n",
       "          [ 2.28390933,  0.        ,  0.        ,  0.        ]]),\n",
       "   array([[[0.00990927]],\n",
       "   \n",
       "          [[0.05586574]],\n",
       "   \n",
       "          [[0.18556038]]]),\n",
       "   array([[-0.11794267],\n",
       "          [-0.12350841],\n",
       "          [ 0.24959069],\n",
       "          [-1.47755568]]))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forward_pass(X_batch, weight_matrices, biases, activations):\n",
    "    # YOUR CODE HERE\n",
    "    isarray(X_batch, 2)\n",
    "    islist(weight_matrices, (np.ndarray, 2))\n",
    "    islist(biases, (np.ndarray, 2))\n",
    "    islist(activations, \"callable\")\n",
    "\n",
    "    output = X_batch\n",
    "    layer_caches = []\n",
    "    for W, b, activation_fn in zip(weight_matrices, biases, activations):\n",
    "        output, cache = layer_forward(output, W, b, activation_fn)\n",
    "        layer_caches.append(cache)\n",
    "    return output, layer_caches\n",
    "\n",
    "def test():\n",
    "    X_batch = np.array([\n",
    "        [1, 2, 3, 4, 8, 4, 1], \n",
    "        [4, 2, 3, 4, 8, 2, 9],\n",
    "        [4, 2, 6, 4, 2, 9, 1],\n",
    "    ])\n",
    "    layer_dims = np.array([7, 4, 1])\n",
    "    weight_matrices = create_weight_matrices(layer_dims)\n",
    "    biases = create_bias_vectors(layer_dims)\n",
    "    activations = [relu_activation, sigmoid_activation]\n",
    "    return forward_pass(X_batch, weight_matrices, biases, activations)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3c (deliverable which has you run a forward pass of your neural network and compute its logistic loss on some output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49732013]\n",
      " [0.49732464]]\n",
      "0.698516803853688\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "output, _ = forward_pass(X_batch, weight_matrices, biases,\n",
    "                         activations)\n",
    "print(output)\n",
    "loss, dL_dg = logistic_loss(output, y_batch)\n",
    "print(loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.3a (deliverable which has you compute the gradient w.r.t. `weight_matrices` and `biases` using a finite differences checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1334,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "with open(\"data/test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "\n",
    "def my_nn_finite_difference_checker(X_batch, y_batch, weight_matrices, biases, activations):\n",
    "    # YOUR CODE HERE\n",
    "    return grad_Ws, grad_bs\n",
    "\n",
    "grad_Ws, grad_bs = my_nn_finite_difference_checker(X_batch, \n",
    "                                                   y_batch, \n",
    "                                                   weight_matrices, \n",
    "                                                   biases, \n",
    "                                                   activations)   \n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(grad_Ws[0])\n",
    "    print()\n",
    "    print(grad_Ws[1])\n",
    "    print()\n",
    "    print(grad_bs[0])\n",
    "    print()\n",
    "    print(grad_bs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.2d (the backward pass!!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`backward_pass`**\n",
    "\n",
    "Parameter 1: `dL_dg`: np.array, matrix, gradient of the loss from logistic loss\n",
    "\n",
    "Parameter 2: `layer_caches`: list of caches (x: np.ndarray,  s: np.ndarray))\n",
    "\n",
    "Returns: (grad_Ws: , grad_bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244499/2777759393.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  noe = [res @ dL_da for res in noe]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1335], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m     loss, dL_dg \u001b[38;5;241m=\u001b[39m logistic_loss(output, y_batch)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backward_pass(dL_dg, layer_caches)\n\u001b[0;32m---> 58\u001b[0m single_exec(test)\n",
      "Cell \u001b[0;32mIn [1317], line 3\u001b[0m, in \u001b[0;36msingle_exec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingle_exec\u001b[39m(func):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m'''Runs a function and removes the funciton scope'''\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     display(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m func\n",
      "Cell \u001b[0;32mIn [1335], line 57\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m output, layer_caches \u001b[38;5;241m=\u001b[39m forward_pass(X_batch, weight_matrices, biases, activations)\n\u001b[1;32m     56\u001b[0m loss, dL_dg \u001b[38;5;241m=\u001b[39m logistic_loss(output, y_batch)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_dg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_caches\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [1335], line 31\u001b[0m, in \u001b[0;36mbackward_pass\u001b[0;34m(dL_dg, layer_caches)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ((x, da_dz, W), dL_da_i) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(layer_caches, dL_da)):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#_, da_dz, _ = layer_caches[i-1]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#print(\"dL_da:\", dL_da_i)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray([ x_i_j \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(da_dz_i) \u001b[38;5;28;01mfor\u001b[39;00m x_i_j \u001b[38;5;129;01min\u001b[39;00m x_i])\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;28;01mfor\u001b[39;00m x_i, da_dz_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x, da_dz)]\n\u001b[0;32m---> 31\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [res \u001b[38;5;241m@\u001b[39m dL_da \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m noe]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(noe)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print(\"--------------------------------\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [1335], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ((x, da_dz, W), dL_da_i) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(layer_caches, dL_da)):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#_, da_dz, _ = layer_caches[i-1]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#print(\"dL_da:\", dL_da_i)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray([ x_i_j \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(da_dz_i) \u001b[38;5;28;01mfor\u001b[39;00m x_i_j \u001b[38;5;129;01min\u001b[39;00m x_i])\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;28;01mfor\u001b[39;00m x_i, da_dz_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x, da_dz)]\n\u001b[0;32m---> 31\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdL_da\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m noe]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(noe)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print(\"--------------------------------\")\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 7)"
     ]
    }
   ],
   "source": [
    "#%%script false --no-raise-error\n",
    "\n",
    "def backward_pass(dL_dg, layer_caches):\n",
    "    # YOUR CODE HERE\n",
    "    '''  for i, cache in enumerate(layer_caches[::-1]):\n",
    "        x, da_dz, W = cache\n",
    "        def z_fn(w):\n",
    "            w_T = np.array([w]).T\n",
    "            return (x @ w_T).T # without constant \"+b\" as it will make no difference here\n",
    "        #v_z = np.vectorize(z, signature=\"(n,m) -> (1, m)\")\n",
    "        dz_dw = x #np.array([gradient_approx(z_fn, w) for w in W.T]).T\n",
    "        display(dz_dw)\n",
    "        #da_dw = np.array([dz_dw_i @ da_dz_i for dz_dw_i, da_dz_i in zip(dz_dw, da_dz)])\n",
    "        print(\"dz_dw:\", dz_dw.shape) \n",
    "        print(\"da_dz:\", da_dz.shape) \n",
    "        #print(\"da_dw:\", da_dw.shape) \n",
    "        print(\"dL_dg:\", dL_dg.shape) \n",
    "\n",
    "    grad_Ws = dL_dg @ x '''\n",
    "\n",
    "    dL_da = [dL_dg]\n",
    "    for i, (x, da_dz, W) in enumerate(layer_caches[::-1]):\n",
    "        W_mul_da_dz = np.array([(W @ da_dz_i).T[0] for da_dz_i in da_dz]).T\n",
    "        dL_da.append((W_mul_da_dz @ dL_da[-1]).T)\n",
    "    dL_da.reverse()\n",
    "\n",
    "    for i, ((x, da_dz, W), dL_da_i) in enumerate(zip(layer_caches, dL_da)):\n",
    "        #_, da_dz, _ = layer_caches[i-1]\n",
    "        #print(\"dL_da:\", dL_da_i)\n",
    "        noe = [np.array([ x_i_j * np.diag(da_dz_i) for x_i_j in x_i]).T  for x_i, da_dz_i in zip(x, da_dz)]\n",
    "        noe = [res @ dL_da for res in noe]\n",
    "        print(noe)\n",
    "        #print(\"--------------------------------\")\n",
    "        print(x)\n",
    "        print(da_dz)\n",
    "        print(dL_da_i)\n",
    "    \n",
    "    \n",
    "\n",
    "    grad_Ws = ...\n",
    "    grad_bs = ...\n",
    "    return grad_Ws, grad_bs\n",
    "\n",
    "def test():\n",
    "    X_batch = np.array([\n",
    "        [1, 2, 3, 4, 8, 4, 1], \n",
    "        [4, 2, 3, 4, 8, 2, 9],\n",
    "        [4, 2, 6, 4, 2, 9, 1],\n",
    "    ])\n",
    "    y_batch = np.array([ 0.99999995, 0.99966556, 0.9998216])\n",
    "    layer_dims = np.array([7, 4, 1])\n",
    "    weight_matrices = create_weight_matrices(layer_dims)\n",
    "    biases = create_bias_vectors(layer_dims)\n",
    "    activations = [relu_activation, sigmoid_activation]\n",
    "    output, layer_caches = forward_pass(X_batch, weight_matrices, biases, activations)\n",
    "    loss, dL_dg = logistic_loss(output, y_batch)\n",
    "    return backward_pass(dL_dg, layer_caches)\n",
    "single_exec(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.3d (test your backward pass! compare it with 1.3a, the gradient computed by the finite difference checker. The answers should match!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244499/596128645.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  noe = [res @ dL_da for res in noe]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,4) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1295], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m output, layer_caches \u001b[38;5;241m=\u001b[39m forward_pass(X_batch, weight_matrices, biases,\n\u001b[1;32m      6\u001b[0m                                     activations)\n\u001b[1;32m      7\u001b[0m loss, dL_dg \u001b[38;5;241m=\u001b[39m logistic_loss(output, y_batch)\n\u001b[0;32m----> 8\u001b[0m grad_Ws, grad_bs \u001b[38;5;241m=\u001b[39m backward_pass(dL_dg, layer_caches)\n\u001b[1;32m     10\u001b[0m grad_bs \u001b[38;5;241m=\u001b[39m grad_Ws \u001b[38;5;66;03m# Slett denne linja!!\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mprintoptions(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn [1294], line 29\u001b[0m, in \u001b[0;36mbackward_pass\u001b[0;34m(dL_dg, layer_caches)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ((x, da_dz, W), dL_da_i) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(layer_caches, dL_da)):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#_, da_dz, _ = layer_caches[i-1]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#print(\"dL_da:\", dL_da_i)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray([ x_i_j \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(da_dz_i) \u001b[38;5;28;01mfor\u001b[39;00m x_i_j \u001b[38;5;129;01min\u001b[39;00m x_i])\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;28;01mfor\u001b[39;00m x_i, da_dz_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x, da_dz)]\n\u001b[0;32m---> 29\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [res \u001b[38;5;241m@\u001b[39m dL_da \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m noe]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(noe)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#print(\"--------------------------------\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [1294], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ((x, da_dz, W), dL_da_i) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(layer_caches, dL_da)):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#_, da_dz, _ = layer_caches[i-1]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#print(\"dL_da:\", dL_da_i)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray([ x_i_j \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(da_dz_i) \u001b[38;5;28;01mfor\u001b[39;00m x_i_j \u001b[38;5;129;01min\u001b[39;00m x_i])\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;28;01mfor\u001b[39;00m x_i, da_dz_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x, da_dz)]\n\u001b[0;32m---> 29\u001b[0m     noe \u001b[38;5;241m=\u001b[39m [\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdL_da\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m noe]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(noe)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#print(\"--------------------------------\")\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,4) into shape (2,)"
     ]
    }
   ],
   "source": [
    "#%%script false --no-raise-error\n",
    "with open(\"data/test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "output, layer_caches = forward_pass(X_batch, weight_matrices, biases,\n",
    "                                    activations)\n",
    "loss, dL_dg = logistic_loss(output, y_batch)\n",
    "grad_Ws, grad_bs = backward_pass(dL_dg, layer_caches)\n",
    "\n",
    "grad_bs = grad_Ws # Slett denne linja!!\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(grad_Ws[0])\n",
    "    print()\n",
    "    print(grad_Ws[1])\n",
    "    print()\n",
    "    print(grad_bs[0])\n",
    "    print()\n",
    "    print(grad_bs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2e (train your neural network on MNIST! save the training and test losses and accuracies at each iteration to use in 1.3e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.271325018672163\n",
      "(1000, 200, 200)\n",
      "(1000, 1, 1)\n",
      "[((784, 200), (1000, 200)), ((200, 1), (1000, 200))]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1099], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     output, layer_caches \u001b[38;5;241m=\u001b[39m forward_pass(X_train, weight_matrices, biases, activations)\n\u001b[1;32m     24\u001b[0m     loss, dL_dg \u001b[38;5;241m=\u001b[39m logistic_loss(output, y_train)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mmean())\n",
      "Cell \u001b[0;32mIn [1094], line 11\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(X_batch, weight_matrices, biases, activations)\u001b[0m\n\u001b[1;32m      9\u001b[0m layer_caches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m W, b, activation_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weight_matrices, biases, activations):\n\u001b[0;32m---> 11\u001b[0m     output, cache \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     layer_caches\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, layer_caches\n",
      "Cell \u001b[0;32mIn [1090], line 11\u001b[0m, in \u001b[0;36mlayer_forward\u001b[0;34m(x, W, b, activation_fn)\u001b[0m\n\u001b[1;32m      9\u001b[0m b \u001b[38;5;241m=\u001b[39m b[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m repeated_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((n, b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), b)\n\u001b[0;32m---> 11\u001b[0m act_fn_in \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m) \u001b[38;5;241m+\u001b[39m repeated_b\n\u001b[1;32m     12\u001b[0m v_activation_fn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(activation_fn, signature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n)->(n), (n, n)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m out, da_dz \u001b[38;5;241m=\u001b[39m v_activation_fn(act_fn_in)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 784)"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_mnist_threes_nines()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ASSERT_OFF = True\n",
    "#display([\"\".join([\".\" if i==0 else \"@\" for i in line]) for line in X_train[0]])\n",
    "#display(\"\".join([\".\" if i==0 else \"@\" for i in X_train[0]]))\n",
    "# Flatten out\n",
    "m, n, o = X_train.shape\n",
    "X_train = X_train.reshape((m, n*o))\n",
    "\n",
    "# Make it easier to debug\n",
    "capacity = 1000\n",
    "X_train = X_train[:capacity]\n",
    "y_train = y_train[:capacity]\n",
    "\n",
    "# Train\n",
    "layer_dims = np.array([784, 200, 1])\n",
    "weight_matrices = create_weight_matrices(layer_dims)\n",
    "biases = create_bias_vectors(layer_dims)\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "out = None\n",
    "for i in range(2):\n",
    "    output, layer_caches = forward_pass(X_train, weight_matrices, biases, activations)\n",
    "    loss, dL_dg = logistic_loss(output, y_train)\n",
    "    print(loss.mean())\n",
    "    grad_Ws, grad_bs = backward_pass(dL_dg, layer_caches)\n",
    "    weight_matrices = [(W.shape, grad_Ws.shape) for W in weight_matrices]\n",
    "    print(weight_matrices)\n",
    "ASSERT_OFF = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3e code answers for i, ii, iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i\n",
    "# Plot the train and test losses from the MNIST network with step size = 0.1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# ii\n",
    "# Plot the train and test accuracies from the MNIST network with step size = 0.1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# iii\n",
    "# Visualize (plot) some images that are misclassified by your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3e iii:\n",
    "Examine the images that your network guesses incorrectly, and explain at a high level\n",
    "what patterns you see in those images.\n",
    "\n",
    "**your answer here**\n",
    "\n",
    "1.3e iv:\n",
    "Rerun the neural network training but now increase the step size to 10.0. What happens?\n",
    "You do not need to include plots here.\n",
    "\n",
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3f (optional) (Train a network to fit 100 random images to the first 100 original labels! How fast can you memorize the dataset?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.rand(100, 784)\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6309f696c0de6f5ad645f2e4da85dbba813c473a657df2c172e75a4d6d29ef16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
